{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gphowBNtIwfi"
   },
   "source": [
    "# Como enseñar a las máquinas a leer y comprender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParlIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ParlAI es un framework de Python que permite la investigación de Inteligencia Artificial de diálogo. Además este framework cuenta con muchos conjuntos de datos populares para realizar múltiples tareas sobre ellos, una amplia gama de ayudantes para crear agentes y capacitarlos en varias tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDJFWJk2JL_C"
   },
   "source": [
    "## Instalación del ParlIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FP5uSxThJJ0p"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q parlai\n",
    "!pip3 install -q subword_nmt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOs6sfkqKpnB"
   },
   "source": [
    "## Importando el Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la evaluación del modelo, se decidió usar el dataset Children’S Book Test (CBT), ya que este era el de menor\n",
    "tamaño y esto facilitaría que el modelo cargue más rápido al momento de entrenar y evaluar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIKL-D12bdtd"
   },
   "source": [
    "### Importamos el Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1eQSLdVDbiZU"
   },
   "outputs": [],
   "source": [
    "from parlai.core.build_data import DownloadableFile\n",
    "import parlai.core.build_data as build_data\n",
    "import os\n",
    "\n",
    "#Descargamos todos los recursos que el dataset requiere en el ParlIA para su uso.\n",
    "RESOURCES = [\n",
    "    DownloadableFile(\n",
    "        'http://parl.ai/downloads/cbt/cbt.tar.gz',\n",
    "        'cbt.tar.gz',\n",
    "        '932df0cadc1337b2a12b4c696b1041c1d1c6d4b6bd319874c6288f02e4a61e92',\n",
    "    )\n",
    "]\n",
    "\n",
    "# Se define el build para descargar y generar los datos necesarios del dataset CBT.\n",
    "def build(opt):\n",
    "    dpath = os.path.join(opt['datapath'], 'CBT')\n",
    "    version = None\n",
    "\n",
    "    if not build_data.built(dpath, version_string=version):\n",
    "        print('[building data: ' + dpath + ']')\n",
    "        if build_data.built(dpath):\n",
    "            # Se elimina los archivos desactualizados de la versión anterior.\n",
    "            build_data.remove_dir(dpath)\n",
    "        build_data.make_dir(dpath)\n",
    "\n",
    "        # Descargamos los datos.\n",
    "        for downloadable_file in RESOURCES:\n",
    "            downloadable_file.download_file(dpath)\n",
    "\n",
    "        # Marcamos los datos como construidos.\n",
    "        build_data.mark_done(dpath, version_string=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:20:10 | Opt:\n",
      "06:20:10 |     allow_missing_init_opts: False\n",
      "06:20:10 |     batchsize: 1\n",
      "06:20:10 |     datapath: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\n",
      "06:20:10 |     datatype: train:ordered\n",
      "06:20:10 |     dict_class: None\n",
      "06:20:10 |     display_add_fields: \n",
      "06:20:10 |     download_path: None\n",
      "06:20:10 |     dynamic_batching: None\n",
      "06:20:10 |     hide_labels: False\n",
      "06:20:10 |     ignore_agent_reply: True\n",
      "06:20:10 |     image_cropsize: 224\n",
      "06:20:10 |     image_mode: raw\n",
      "06:20:10 |     image_size: 256\n",
      "06:20:10 |     init_model: None\n",
      "06:20:10 |     init_opt: None\n",
      "06:20:10 |     is_debug: False\n",
      "06:20:10 |     loglevel: info\n",
      "06:20:10 |     max_display_len: 1000\n",
      "06:20:10 |     model: None\n",
      "06:20:10 |     model_file: None\n",
      "06:20:10 |     multitask_weights: [1]\n",
      "06:20:10 |     mutators: None\n",
      "06:20:10 |     num_examples: 1\n",
      "06:20:10 |     override: \"{'task': 'cbt', 'num_examples': 1}\"\n",
      "06:20:10 |     parlai_home: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\n",
      "06:20:10 |     starttime: Dec05_06-20\n",
      "06:20:10 |     task: cbt\n",
      "06:20:10 |     verbose: False\n",
      "06:20:11 | creating task(s): cbt\n",
      "06:20:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_train.txt\n",
      "06:20:18 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_train.txt\n",
      "06:21:01 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_train.txt\n",
      "06:21:08 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_train.txt\n",
      "\u001b[1;31m- - - NEW EPISODE: cbt:NE - - -\u001b[0;0m\n",
      "\u001b[0mFill in the blank in the last sentence.\n",
      "Some were abroad ; several were ill ; a few were in prison among the Saracens ; others were captives in the dens of ogres .\n",
      "The end of it was that the king and queen had to sit down alone , one at each end of a very long table , arrayed with plates and glasses for a hundred guests -- for a hundred guests who never came !\n",
      "`` Any soup , my dear ? ''\n",
      "shouted the king , through a speaking-trumpet ; when , suddenly , the air was filled with a sound like the rustling of the wings of birds .\n",
      "Flitter , flitter , flutter , went the noise ; and when the queen looked up , lo and behold !\n",
      "on every seat was a lovely fairy , dressed in green , each with a most interesting-looking parcel in her hand .\n",
      "Do n't you like opening parcels ?\n",
      "The king did , and he was most friendly and polite to the fairies .\n",
      "But the queen , though she saw them distinctly , took no notice of them .\n",
      "You see , she did not believe in fairies , nor in her own eyes , when she saw them .\n",
      "So she talked across the fairies to the king , just as if they had not been there ; but the king behaved as politely as if they were real -- which , of course , they were .\n",
      "When dinner was over , and when the nurse had brought in the baby , all the fairies gave him the most magnificent presents .\n",
      "One offered a purse which could never be empty ; and one a pair of seven-leagued boots ; and another a cap of darkness , that nobody might see the prince when he put it on ; and another a wishing-cap ; and another a carpet , on which , when he sat , he was carried wherever he wished to find himself .\n",
      "Another made him beautiful for ever ; and another , brave ; and another , lucky : but the last fairy of all , a cross old thing , crept up and said , `` My child , you shall be too clever ! ''\n",
      "This fairy 's gift would have pleased the queen , if she had believed in it , more than anything else , because she was so clever herself .\n",
      "But she took no notice at all ; and the fairies went each to her own country , and none of them stayed there at the palace , where nobody believed in them , except the king , a little .\n",
      "But the queen tossed all their nice boots and caps , carpets , purses , swords , and all , away into a dark lumber-room ; for , of course , she thought that they were all nonsense , and merely old rubbish out of books , or pantomime `` properties . ''\n",
      "CHAPTER II .\n",
      "-LCB- Chapter heading picture : p9.jpg -RCB- Prince Prigio and his Family .\n",
      "Well , the little prince grew up .\n",
      "I think I 've told you that his name was XXXXX -- did I not ?\u001b[0;0m\n",
      "   \u001b[1;94mPrigio\u001b[0;0m\n",
      "06:21:34 | loaded 669343 episodes with a total of 669343 examples\n"
     ]
    }
   ],
   "source": [
    "# El script display_data se usa para mostrar el contenido de una tarea en particular.\n",
    "# Mostramos un ejemplo de los datos del train\n",
    "from parlai.scripts.display_data import DisplayData\n",
    "# Num_example define el número de ejemplos a mostrar y task es la tarea del dataset, en este caso de cbt.\n",
    "DisplayData.main(task='cbt', num_examples=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tqVe18TcRi7"
   },
   "source": [
    "### Implementamos los agentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos los agentes para que comprendan la estructura de las tareas y que sean capaces de representarlas.\n",
    "En este caso crearemos cuatro agentes que llamaremos profesores para que cada uno realice tareas específicas diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r2J9wku5cT1U"
   },
   "outputs": [],
   "source": [
    "# Importamos FbDeprecatedDialogTeacher, MultiTaskTeacher para que nuestros agentes creados hereden sus métodos.\n",
    "from parlai.core.teachers import FbDeprecatedDialogTeacher, MultiTaskTeacher\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Creamos el path para devolvernos a la ruta de los archivos de datos correctos del dataset.\n",
    "def _path(task, opt):\n",
    "    # Genera los datos si no existen.\n",
    "    build(opt)\n",
    "    suffix = ''\n",
    "    dt = opt['datatype'].split(':')[0]\n",
    "    if dt == 'train':\n",
    "        suffix = 'train'\n",
    "    elif dt == 'test':\n",
    "        suffix = 'test_2500ex'\n",
    "    elif dt == 'valid':\n",
    "        suffix = 'valid_2000ex'\n",
    "\n",
    "    return os.path.join(\n",
    "        opt['datapath'], 'CBT', 'CBTest', 'data', task + '_' + suffix + '.txt'\n",
    "    )\n",
    "\n",
    "# Se crean los profesores para que entiendan las tareas y sean capaces de representarlas.\n",
    "class NETeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_NE', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "class CNTeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_CN', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "class VTeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_V', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "class PTeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_P', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "# De forma predeterminada, este último profesor entrena a todas las tareas a la vez.\n",
    "\n",
    "class DefaultTeacher(MultiTaskTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt = copy.deepcopy(opt)\n",
    "        opt['task'] = 'cbt:NE,cbt:CN,cbt:V,cbt:P'\n",
    "        super().__init__(opt, shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBfO781TltNR"
   },
   "source": [
    "# Creando nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un modelo seq2seq, que va a consistir en un encoder y decoder cada uno conteniendo una capa LSTM.\n",
    "La clase TorchGeneratorAgent manejará las características comunes de un decoder, como la decodificación forzada y la búsqueda en haz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hVrZh-T903wh"
   },
   "outputs": [],
   "source": [
    "# Importamos torch.nn para la creación de la red neuronal\n",
    "import torch.nn as nn\n",
    "# Importamos torch.nn.functional que nos servirá para implementar capas que no tienen parámatros\n",
    "import torch.nn.functional as F\n",
    "import parlai.core.torch_generator_agent as tga\n",
    "\n",
    "# Definimos el encoder \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    #Consta de una capa de incrustación y un LSTM de 1 capa con el\n",
    "    #tamaño oculto especificado.\n",
    "    \n",
    "    #Inicialización.\n",
    "    def __init__(self, embeddings, hidden_size):\n",
    "        \n",
    "        # Llamamos a super en todos los nn.Modules para que lo herede.\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        # Se definen los parámetros para la capa LSTM del encoder.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "                   \n",
    "        #Realice el forward pass para el codificador.\n",
    "        \n",
    "        #La entrada input_tokens, son los tokens de contexto dados\n",
    "        embedded = self.embeddings(input_tokens)\n",
    "        # Se devuelven los estados ocultos y de la celda LSTM\n",
    "        _output, hidden = self.lstm(embedded)\n",
    "        return hidden\n",
    "\n",
    "# Definimos el decoder\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    #Consta de una capa de incrustación y un LSTM de 1 capa con el\n",
    "    #tamaño oculto especificado.\n",
    "   \n",
    "    #El decodificador permite la decodificación incremental ingiriendo el\n",
    "    #estado incremental actual en cada pasada hacia adelante.\n",
    "  \n",
    "    #Inicialización.\n",
    "    def __init__(self, embeddings, hidden_size):\n",
    "        \n",
    "        # Llamamos a super en todos los nn.Modules para que lo herede.\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        # Se definen los parámetros para la capa LSTM del decoder.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input, encoder_state, incr_state=None):\n",
    "        \n",
    "        #Realice el forward pass para el decodificador.\n",
    "        \n",
    "        #La entrada son los tokens generados por el decodificador\n",
    "        embedded = self.embeddings(input)\n",
    "        if incr_state is None:\n",
    "            # Sembramos el LSTM con el estado oculto del decodificador.\n",
    "            state = encoder_state\n",
    "        else:\n",
    "            # Reutilizamos el estado del decodificador existente\n",
    "            state = incr_state\n",
    "\n",
    "        # Obtenemos la nueva salida y el estado incremental del decodificador\n",
    "        output, incr_state = self.lstm(embedded, state)\n",
    "\n",
    "        return output, incr_state\n",
    "\n",
    "# Implementa los métodos de TorchGeneratorModel para reordenar los estados del codificador y los estados incrementales del\n",
    "# decodificador. Crea una instancia y también define la capa de salida final.\n",
    "class ExampleModel(tga.TorchGeneratorModel):\n",
    "   \n",
    "    #Inicialización.\n",
    "    def __init__(self, dictionary, hidden_size=1024):\n",
    "        super().__init__(\n",
    "            padding_idx=dictionary[dictionary.null_token],\n",
    "            start_idx=dictionary[dictionary.start_token],\n",
    "            end_idx=dictionary[dictionary.end_token],\n",
    "            unknown_idx=dictionary[dictionary.unk_token],\n",
    "        )\n",
    "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
    "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
    "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
    "\n",
    "    def output(self, decoder_output):\n",
    "        \n",
    "        #Realiza la salida final -> transformación logits.\n",
    "        \n",
    "        return F.linear(decoder_output, self.embeddings.weight)\n",
    "\n",
    "    def reorder_encoder_states(self, encoder_states, indices):\n",
    "        \n",
    "        #Reordena los estados del codificador para seleccionar solo los índices de lote dados.\n",
    "        #Se indexa la selección en la dimensión del lote.\n",
    "        h, c = encoder_states\n",
    "        return h[:, indices, :], c[:, indices, :]\n",
    "\n",
    "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
    "        # Método es implementado para reducir la complejidad de generación.\n",
    "        h, c = incr_state\n",
    "        return h[:, indices, :], c[:, indices, :]\n",
    "\n",
    "# Creamos el modelo Seq2seq que hereda de TorchGeneratorAgent\n",
    "@register_agent(\"my_first_lstm\")\n",
    "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
    "    \n",
    "    @classmethod\n",
    "    def add_cmdline_args(cls, argparser, partial_opt):\n",
    "\n",
    "        # Agrega todos los argumentos de TorchGeneratorAgent\n",
    "        super().add_cmdline_args(argparser)\n",
    "\n",
    "        # Agregamos argumentos personalizados solo para este modelo.\n",
    "        group = argparser.add_argument_group('Example TGA Agent')\n",
    "        group.add_argument(\n",
    "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
    "        )\n",
    "\n",
    "    # Se construye el modelo.\n",
    "    def build_model(self):\n",
    "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
    "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMQ4N0Bfm67-"
   },
   "source": [
    "## Entrenando el Modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con el dataset CBT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lfjG72qcnGch",
    "outputId": "66dab851-05e3-433e-d8b4-7d2287b8f7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:49:52 | building dictionary first...\n",
      "03:49:52 | No model with opt yet at: my_first_lstm/model(.opt)\n",
      "03:49:52 | Using CUDA\n",
      "03:49:52 | loading dictionary from my_first_lstm/model.dict\n",
      "03:49:52 | num words = 51210\n",
      "03:49:52 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "03:49:52 | Opt:\n",
      "03:49:52 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "03:49:52 |     adam_eps: 1e-08\n",
      "03:49:52 |     add_p1_after_newln: False\n",
      "03:49:52 |     aggregate_micro: False\n",
      "03:49:52 |     allow_missing_init_opts: False\n",
      "03:49:52 |     batchsize: 1\n",
      "03:49:52 |     beam_block_full_context: True\n",
      "03:49:52 |     beam_block_list_filename: None\n",
      "03:49:52 |     beam_block_ngram: -1\n",
      "03:49:52 |     beam_context_block_ngram: -1\n",
      "03:49:52 |     beam_delay: 30\n",
      "03:49:52 |     beam_length_penalty: 0.65\n",
      "03:49:52 |     beam_min_length: 1\n",
      "03:49:52 |     beam_size: 1\n",
      "03:49:52 |     betas: '(0.9, 0.999)'\n",
      "03:49:52 |     bpe_add_prefix_space: None\n",
      "03:49:52 |     bpe_debug: False\n",
      "03:49:52 |     bpe_dropout: None\n",
      "03:49:52 |     bpe_merge: None\n",
      "03:49:52 |     bpe_vocab: None\n",
      "03:49:52 |     compute_tokenized_bleu: False\n",
      "03:49:52 |     datapath: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\n",
      "03:49:52 |     datatype: train\n",
      "03:49:52 |     delimiter: '\\n'\n",
      "03:49:52 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "03:49:52 |     dict_endtoken: __end__\n",
      "03:49:52 |     dict_file: my_first_lstm/model.dict\n",
      "03:49:52 |     dict_include_test: False\n",
      "03:49:52 |     dict_include_valid: False\n",
      "03:49:52 |     dict_initpath: None\n",
      "03:49:52 |     dict_language: english\n",
      "03:49:52 |     dict_loaded: True\n",
      "03:49:52 |     dict_lower: False\n",
      "03:49:52 |     dict_max_ngram_size: -1\n",
      "03:49:52 |     dict_maxexs: -1\n",
      "03:49:52 |     dict_maxtokens: -1\n",
      "03:49:52 |     dict_minfreq: 0\n",
      "03:49:52 |     dict_nulltoken: __null__\n",
      "03:49:52 |     dict_starttoken: __start__\n",
      "03:49:52 |     dict_textfields: text,labels\n",
      "03:49:52 |     dict_tokenizer: re\n",
      "03:49:52 |     dict_unktoken: __unk__\n",
      "03:49:52 |     display_examples: False\n",
      "03:49:52 |     download_path: None\n",
      "03:49:52 |     dynamic_batching: None\n",
      "03:49:52 |     embedding_projection: random\n",
      "03:49:52 |     embedding_type: random\n",
      "03:49:52 |     eval_batchsize: None\n",
      "03:49:52 |     eval_dynamic_batching: None\n",
      "03:49:52 |     evaltask: None\n",
      "03:49:52 |     final_extra_opt: \n",
      "03:49:52 |     force_fp16_tokens: False\n",
      "03:49:52 |     fp16: False\n",
      "03:49:52 |     fp16_impl: safe\n",
      "03:49:52 |     gpu: -1\n",
      "03:49:52 |     gradient_clip: 0.1\n",
      "03:49:52 |     hidden_size: 1024\n",
      "03:49:52 |     hide_labels: False\n",
      "03:49:52 |     history_add_global_end_token: None\n",
      "03:49:52 |     history_reversed: False\n",
      "03:49:52 |     history_size: -1\n",
      "03:49:52 |     image_cropsize: 224\n",
      "03:49:52 |     image_mode: raw\n",
      "03:49:52 |     image_size: 256\n",
      "03:49:52 |     inference: greedy\n",
      "03:49:52 |     init_model: None\n",
      "03:49:52 |     init_opt: None\n",
      "03:49:52 |     interactive_mode: False\n",
      "03:49:52 |     invsqrt_lr_decay_gamma: -1\n",
      "03:49:52 |     is_debug: False\n",
      "03:49:52 |     label_truncate: None\n",
      "03:49:52 |     learningrate: 1\n",
      "03:49:52 |     load_from_checkpoint: True\n",
      "03:49:52 |     log_every_n_secs: -1\n",
      "03:49:52 |     log_every_n_steps: 50\n",
      "03:49:52 |     loglevel: info\n",
      "03:49:52 |     lr_scheduler: reduceonplateau\n",
      "03:49:52 |     lr_scheduler_decay: 0.5\n",
      "03:49:52 |     lr_scheduler_patience: 3\n",
      "03:49:52 |     max_train_steps: -1\n",
      "03:49:52 |     max_train_time: 60.0\n",
      "03:49:52 |     metrics: default\n",
      "03:49:52 |     model: my_first_lstm\n",
      "03:49:52 |     model_file: my_first_lstm/model\n",
      "03:49:52 |     momentum: 0\n",
      "03:49:52 |     multitask_weights: [1]\n",
      "03:49:52 |     mutators: None\n",
      "03:49:52 |     nesterov: True\n",
      "03:49:52 |     no_cuda: False\n",
      "03:49:52 |     num_epochs: -1\n",
      "03:49:52 |     num_workers: 0\n",
      "03:49:52 |     nus: (0.7,)\n",
      "03:49:52 |     optimizer: sgd\n",
      "03:49:52 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'cbt', 'batchsize': 1, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
      "03:49:52 |     parlai_home: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\n",
      "03:49:52 |     person_tokens: False\n",
      "03:49:52 |     rank_candidates: False\n",
      "03:49:52 |     save_after_valid: False\n",
      "03:49:52 |     save_every_n_secs: -1\n",
      "03:49:52 |     short_final_eval: False\n",
      "03:49:52 |     skip_generation: False\n",
      "03:49:52 |     special_tok_lst: None\n",
      "03:49:52 |     split_lines: False\n",
      "03:49:52 |     starttime: Dec05_03-49\n",
      "03:49:52 |     task: cbt\n",
      "03:49:52 |     temperature: 1.0\n",
      "03:49:52 |     tensorboard_log: False\n",
      "03:49:52 |     tensorboard_logdir: None\n",
      "03:49:52 |     text_truncate: None\n",
      "03:49:52 |     topk: 10\n",
      "03:49:52 |     topp: 0.9\n",
      "03:49:52 |     truncate: -1\n",
      "03:49:52 |     update_freq: 1\n",
      "03:49:52 |     use_reply: label\n",
      "03:49:52 |     validation_cutoff: 1.0\n",
      "03:49:53 |     validation_every_n_epochs: -1\n",
      "03:49:53 |     validation_every_n_secs: 10.0\n",
      "03:49:53 |     validation_every_n_steps: -1\n",
      "03:49:53 |     validation_max_exs: -1\n",
      "03:49:53 |     validation_metric: accuracy\n",
      "03:49:53 |     validation_metric_mode: None\n",
      "03:49:53 |     validation_patience: 10\n",
      "03:49:53 |     validation_share_agent: False\n",
      "03:49:53 |     verbose: False\n",
      "03:49:53 |     wandb_entity: None\n",
      "03:49:53 |     wandb_log: False\n",
      "03:49:53 |     wandb_name: None\n",
      "03:49:53 |     wandb_project: None\n",
      "03:49:53 |     warmup_rate: 0.0001\n",
      "03:49:53 |     warmup_updates: -1\n",
      "03:49:53 |     weight_decay: None\n",
      "03:49:53 | creating task(s): cbt\n",
      "03:49:53 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_train.txt\n",
      "03:49:59 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_train.txt\n",
      "03:50:09 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_train.txt\n",
      "03:50:15 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_train.txt\n",
      "03:50:41 | training...\n",
      "03:50:52 | time:10s total_exs:50 total_steps:50 epochs:0.00\n",
      "           clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
      "   all    563.1     1   557  2729       0          0 4.899   50  175.8    .4118     2 12.54   1     2 9.799       0   \n",
      "   cbt:CN 602.9                         0          0         10                     2 13.46                       0   \n",
      "   cbt:NE 573.8                         0          0          5                     2 15.46                       0   \n",
      "   cbt:P  580.1                         0          0         19                     2 9.126                       0   \n",
      "   cbt:V  495.6                         0          0         16                     2 12.11                       0   \n",
      "           ltrunclen     ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
      "   all             0 1513935      .2605         0                   50  559 2739 4.899  \n",
      "   cbt:CN          0  701097      .2500         0                                       \n",
      "   cbt:NE          0 5163857      .2000         0                                       \n",
      "   cbt:P           0    9188      .3421         0                                       \n",
      "   cbt:V           0  181597      .2500         0\n",
      "\n",
      "03:50:52 | creating task(s): cbt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_valid_2000ex.txt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_valid_2000ex.txt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_valid_2000ex.txt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_valid_2000ex.txt\n",
      "03:50:52 | running eval: valid\n",
      "04:11:08 | eval completed in 1216.06s\n",
      "04:11:08 | \u001b[1mvalid:\n",
      "           accuracy   bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs      f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all      .000625 6.25e-13 512.3 512.3  3370       0          0 6.579 8000 .000625    .2580 2.013 9.236   1 2.013 13.24   \n",
      "   cbt:CN     .0010    1e-12 522.1                   0          0       2000   .0010          2.004 11.07                   \n",
      "   cbt:NE         0        0   489                   0          0       2000       0          2.047 10.06                   \n",
      "   cbt:P      .0015  1.5e-12 528.9                   0          0       2000   .0015              2 5.784                   \n",
      "   cbt:V          0        0 509.2                   0          0       2000       0              2 10.04                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "   all          0          0 27608      .3525   .000625                   50 514.3 3384  \n",
      "   cbt:CN       0          0 63955      .3403     .0010                                  \n",
      "   cbt:NE       0          0 23331      .2694         0                                  \n",
      "   cbt:P        0          0   325      .4740     .0015                                  \n",
      "   cbt:V        0          0 22820      .3262         0\n",
      "\u001b[0m\n",
      "04:11:08 | \u001b[1;32mnew best accuracy: 0.000625\u001b[0m\n",
      "04:11:08 | saving best valid model: my_first_lstm/model\n",
      "04:11:09 | max_train_time elapsed:1227.7263507843018s\n",
      "04:11:10 | Using CUDA\n",
      "04:11:10 | loading dictionary from my_first_lstm/model.dict\n",
      "04:11:10 | num words = 51210\n",
      "04:11:11 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "04:11:11 | Loading existing model params from my_first_lstm/model\n",
      "04:11:11 | creating task(s): cbt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_valid_2000ex.txt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_valid_2000ex.txt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_valid_2000ex.txt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_valid_2000ex.txt\n",
      "04:11:12 | running eval: valid\n",
      "04:31:07 | eval completed in 1195.34s\n",
      "04:31:07 | \u001b[1mvalid:\n",
      "           accuracy   bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs      f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all      .000625 6.25e-13 512.3 512.3  3429       0          0 6.693 8000 .000625    .1936 2.013 9.236   1 2.013 13.47   \n",
      "   cbt:CN     .0010    1e-12 522.1                   0          0       2000   .0010          2.004 11.07                   \n",
      "   cbt:NE         0        0   489                   0          0       2000       0          2.047 10.06                   \n",
      "   cbt:P      .0015  1.5e-12 528.9                   0          0       2000   .0015              2 5.784                   \n",
      "   cbt:V          0        0 509.2                   0          0       2000       0              2 10.04                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "   all          0          0 27608      .3525   .000625                   50 514.3 3442  \n",
      "   cbt:CN       0          0 63955      .3403     .0010                                  \n",
      "   cbt:NE       0          0 23331      .2694         0                                  \n",
      "   cbt:P        0          0   325      .4740     .0015                                  \n",
      "   cbt:V        0          0 22820      .3262         0\n",
      "\u001b[0m\n",
      "04:31:07 | creating task(s): cbt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_test_2500ex.txt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_test_2500ex.txt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_test_2500ex.txt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_test_2500ex.txt\n",
      "04:31:08 | running eval: test\n",
      "04:56:25 | eval completed in 1517.25s\n",
      "04:56:25 | \u001b[1mtest:\n",
      "           accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs       f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all        .0003 3.368e-13 524.4 524.4  3456       0          0 6.591 10000 .0003667    .1937 2.029 9.459   1 2.029 13.37   \n",
      "   cbt:CN     .0008     8e-13 539.3                   0          0        2500    .0008          2.004 11.03                   \n",
      "   cbt:NE         0 1.472e-13 503.7                   0          0        2500 .0002667           2.11 10.97                   \n",
      "   cbt:P      .0004     4e-13 537.1                   0          0        2500    .0004              2  5.86                   \n",
      "   cbt:V          0         0 517.3                   0          0        2500        0          2.001 9.969                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "   all          0          0 35458      .3458     .0003                   50 526.4 3469  \n",
      "   cbt:CN       0          0 61960      .3154     .0008                                  \n",
      "   cbt:NE       0          0 58171      .2527         0                                  \n",
      "   cbt:P        0          0 350.8      .4698     .0004                                  \n",
      "   cbt:V        0          0 21349      .3455         0\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'cbt:NE/exs': SumMetric(2000),\n",
       "  'exs': SumMetric(8000),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0),\n",
       "  'cbt:NE/bleu-4': BleuMetric(0),\n",
       "  'cbt:NE/clen': AverageMetric(489),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.047),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.06),\n",
       "  'cbt:NE/ppl': PPLMetric(2.333e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.2694),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2000),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0.001),\n",
       "  'cbt:CN/f1': F1Metric(0.001),\n",
       "  'cbt:CN/bleu-4': BleuMetric(1e-12),\n",
       "  'cbt:CN/clen': AverageMetric(522.1),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.07),\n",
       "  'cbt:CN/ppl': PPLMetric(6.395e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3403),\n",
       "  'cbt:CN/token_em': AverageMetric(0.001),\n",
       "  'cbt:V/exs': SumMetric(2000),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(509.2),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(10.04),\n",
       "  'cbt:V/ppl': PPLMetric(2.282e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.3262),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2000),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0015),\n",
       "  'cbt:P/f1': F1Metric(0.0015),\n",
       "  'cbt:P/bleu-4': BleuMetric(1.5e-12),\n",
       "  'cbt:P/clen': AverageMetric(528.9),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(5.784),\n",
       "  'cbt:P/ppl': PPLMetric(325),\n",
       "  'cbt:P/token_acc': AverageMetric(0.474),\n",
       "  'cbt:P/token_em': AverageMetric(0.0015),\n",
       "  'accuracy': MacroAverageMetric(0.000625),\n",
       "  'f1': MacroAverageMetric(0.000625),\n",
       "  'bleu-4': MacroAverageMetric(6.25e-13),\n",
       "  'clen': MacroAverageMetric(512.3),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.013),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(9.236),\n",
       "  'ppl': MacroAverageMetric(2.761e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3525),\n",
       "  'token_em': MacroAverageMetric(0.000625),\n",
       "  'exps': GlobalTimerMetric(6.693),\n",
       "  'ltpb': GlobalAverageMetric(2.013),\n",
       "  'ltps': GlobalTimerMetric(13.47),\n",
       "  'ctpb': GlobalAverageMetric(512.3),\n",
       "  'ctps': GlobalTimerMetric(3429),\n",
       "  'tpb': GlobalAverageMetric(514.3),\n",
       "  'tps': GlobalTimerMetric(3442),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1936),\n",
       "  'total_train_updates': GlobalFixedMetric(50)},\n",
       " {'cbt:NE/exs': SumMetric(2500),\n",
       "  'exs': SumMetric(1e+04),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0.0002667),\n",
       "  'cbt:NE/bleu-4': BleuMetric(1.472e-13),\n",
       "  'cbt:NE/clen': AverageMetric(503.7),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.11),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.97),\n",
       "  'cbt:NE/ppl': PPLMetric(5.817e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.2527),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2500),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0.0008),\n",
       "  'cbt:CN/f1': F1Metric(0.0008),\n",
       "  'cbt:CN/bleu-4': BleuMetric(8e-13),\n",
       "  'cbt:CN/clen': AverageMetric(539.3),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.03),\n",
       "  'cbt:CN/ppl': PPLMetric(6.196e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3154),\n",
       "  'cbt:CN/token_em': AverageMetric(0.0008),\n",
       "  'cbt:V/exs': SumMetric(2500),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(517.3),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2.001),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(9.969),\n",
       "  'cbt:V/ppl': PPLMetric(2.135e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.3455),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2500),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0004),\n",
       "  'cbt:P/f1': F1Metric(0.0004),\n",
       "  'cbt:P/bleu-4': BleuMetric(4e-13),\n",
       "  'cbt:P/clen': AverageMetric(537.1),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(5.86),\n",
       "  'cbt:P/ppl': PPLMetric(350.8),\n",
       "  'cbt:P/token_acc': AverageMetric(0.4698),\n",
       "  'cbt:P/token_em': AverageMetric(0.0004),\n",
       "  'accuracy': MacroAverageMetric(0.0003),\n",
       "  'f1': MacroAverageMetric(0.0003667),\n",
       "  'bleu-4': MacroAverageMetric(3.368e-13),\n",
       "  'clen': MacroAverageMetric(524.4),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.029),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(9.459),\n",
       "  'ppl': MacroAverageMetric(3.546e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3458),\n",
       "  'token_em': MacroAverageMetric(0.0003),\n",
       "  'exps': GlobalTimerMetric(6.591),\n",
       "  'ltpb': GlobalAverageMetric(2.029),\n",
       "  'ltps': GlobalTimerMetric(13.37),\n",
       "  'ctpb': GlobalAverageMetric(524.4),\n",
       "  'ctps': GlobalTimerMetric(3456),\n",
       "  'tpb': GlobalAverageMetric(526.4),\n",
       "  'tps': GlobalTimerMetric(3469),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1937),\n",
       "  'total_train_updates': GlobalFixedMetric(50)})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importamos TrainModel de ParlIA para entrenar el modelo seq2seq creado\n",
    "from parlai.scripts.train_model import TrainModel\n",
    "from parlai.core.agents import create_agent\n",
    "\n",
    "TrainModel.main(\n",
    "    model='my_first_lstm',\n",
    "    model_file='my_first_lstm/model',\n",
    "    task='cbt',\n",
    "    # Entrenamos el modelo con un lote\n",
    "    batchsize=1,\n",
    "    validation_every_n_secs=10,\n",
    "    max_train_time=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:15:31 | building dictionary first...\n",
      "05:15:31 | \u001b[33mOverriding opt[\"batchsize\"] to 3 (previously: 1)\u001b[0m\n",
      "05:15:31 | Using CUDA\n",
      "05:15:31 | loading dictionary from my_first_lstm/model.dict\n",
      "05:15:31 | num words = 51210\n",
      "05:15:32 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "05:15:32 | Loading existing model params from my_first_lstm/model\n",
      "05:15:32 | Opt:\n",
      "05:15:32 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "05:15:32 |     adam_eps: 1e-08\n",
      "05:15:32 |     add_p1_after_newln: False\n",
      "05:15:32 |     aggregate_micro: False\n",
      "05:15:32 |     allow_missing_init_opts: False\n",
      "05:15:32 |     batchsize: 3\n",
      "05:15:32 |     beam_block_full_context: True\n",
      "05:15:32 |     beam_block_list_filename: None\n",
      "05:15:32 |     beam_block_ngram: -1\n",
      "05:15:32 |     beam_context_block_ngram: -1\n",
      "05:15:32 |     beam_delay: 30\n",
      "05:15:32 |     beam_length_penalty: 0.65\n",
      "05:15:32 |     beam_min_length: 1\n",
      "05:15:32 |     beam_size: 1\n",
      "05:15:32 |     betas: '[0.9, 0.999]'\n",
      "05:15:32 |     bpe_add_prefix_space: None\n",
      "05:15:32 |     bpe_debug: False\n",
      "05:15:32 |     bpe_dropout: None\n",
      "05:15:32 |     bpe_merge: None\n",
      "05:15:32 |     bpe_vocab: None\n",
      "05:15:32 |     compute_tokenized_bleu: False\n",
      "05:15:32 |     datapath: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\n",
      "05:15:32 |     datatype: train\n",
      "05:15:32 |     delimiter: '\\n'\n",
      "05:15:32 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "05:15:32 |     dict_endtoken: __end__\n",
      "05:15:32 |     dict_file: my_first_lstm/model.dict\n",
      "05:15:32 |     dict_include_test: False\n",
      "05:15:32 |     dict_include_valid: False\n",
      "05:15:32 |     dict_initpath: None\n",
      "05:15:32 |     dict_language: english\n",
      "05:15:32 |     dict_loaded: True\n",
      "05:15:32 |     dict_lower: False\n",
      "05:15:32 |     dict_max_ngram_size: -1\n",
      "05:15:32 |     dict_maxexs: -1\n",
      "05:15:32 |     dict_maxtokens: -1\n",
      "05:15:32 |     dict_minfreq: 0\n",
      "05:15:32 |     dict_nulltoken: __null__\n",
      "05:15:32 |     dict_starttoken: __start__\n",
      "05:15:32 |     dict_textfields: text,labels\n",
      "05:15:32 |     dict_tokenizer: re\n",
      "05:15:32 |     dict_unktoken: __unk__\n",
      "05:15:32 |     display_examples: False\n",
      "05:15:32 |     download_path: None\n",
      "05:15:32 |     dynamic_batching: None\n",
      "05:15:32 |     embedding_projection: random\n",
      "05:15:32 |     embedding_type: random\n",
      "05:15:32 |     eval_batchsize: None\n",
      "05:15:32 |     eval_dynamic_batching: None\n",
      "05:15:32 |     evaltask: None\n",
      "05:15:32 |     final_extra_opt: \n",
      "05:15:32 |     force_fp16_tokens: False\n",
      "05:15:32 |     fp16: False\n",
      "05:15:32 |     fp16_impl: safe\n",
      "05:15:32 |     gpu: -1\n",
      "05:15:32 |     gradient_clip: 0.1\n",
      "05:15:32 |     hidden_size: 1024\n",
      "05:15:32 |     hide_labels: False\n",
      "05:15:32 |     history_add_global_end_token: None\n",
      "05:15:32 |     history_reversed: False\n",
      "05:15:32 |     history_size: -1\n",
      "05:15:32 |     image_cropsize: 224\n",
      "05:15:32 |     image_mode: raw\n",
      "05:15:32 |     image_size: 256\n",
      "05:15:32 |     inference: greedy\n",
      "05:15:32 |     init_model: None\n",
      "05:15:32 |     init_opt: None\n",
      "05:15:32 |     interactive_mode: False\n",
      "05:15:32 |     invsqrt_lr_decay_gamma: -1\n",
      "05:15:32 |     is_debug: False\n",
      "05:15:32 |     label_truncate: None\n",
      "05:15:32 |     learningrate: 1\n",
      "05:15:32 |     load_from_checkpoint: True\n",
      "05:15:32 |     log_every_n_secs: -1\n",
      "05:15:32 |     log_every_n_steps: 50\n",
      "05:15:32 |     loglevel: info\n",
      "05:15:32 |     lr_scheduler: reduceonplateau\n",
      "05:15:32 |     lr_scheduler_decay: 0.5\n",
      "05:15:32 |     lr_scheduler_patience: 3\n",
      "05:15:32 |     max_train_steps: -1\n",
      "05:15:32 |     max_train_time: 60.0\n",
      "05:15:32 |     metrics: default\n",
      "05:15:32 |     model: my_first_lstm\n",
      "05:15:32 |     model_file: my_first_lstm/model\n",
      "05:15:32 |     momentum: 0\n",
      "05:15:32 |     multitask_weights: [1]\n",
      "05:15:32 |     mutators: None\n",
      "05:15:32 |     nesterov: True\n",
      "05:15:32 |     no_cuda: False\n",
      "05:15:32 |     num_epochs: -1\n",
      "05:15:32 |     num_workers: 0\n",
      "05:15:32 |     nus: [0.7]\n",
      "05:15:32 |     optimizer: sgd\n",
      "05:15:32 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'cbt', 'batchsize': 3, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
      "05:15:32 |     parlai_home: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\n",
      "05:15:32 |     person_tokens: False\n",
      "05:15:32 |     rank_candidates: False\n",
      "05:15:32 |     save_after_valid: False\n",
      "05:15:32 |     save_every_n_secs: -1\n",
      "05:15:32 |     short_final_eval: False\n",
      "05:15:32 |     skip_generation: False\n",
      "05:15:32 |     special_tok_lst: None\n",
      "05:15:32 |     split_lines: False\n",
      "05:15:32 |     starttime: Dec05_03-49\n",
      "05:15:32 |     task: cbt\n",
      "05:15:32 |     temperature: 1.0\n",
      "05:15:32 |     tensorboard_log: False\n",
      "05:15:32 |     tensorboard_logdir: None\n",
      "05:15:32 |     text_truncate: None\n",
      "05:15:32 |     topk: 10\n",
      "05:15:32 |     topp: 0.9\n",
      "05:15:32 |     truncate: -1\n",
      "05:15:32 |     update_freq: 1\n",
      "05:15:32 |     use_reply: label\n",
      "05:15:32 |     validation_cutoff: 1.0\n",
      "05:15:32 |     validation_every_n_epochs: -1\n",
      "05:15:32 |     validation_every_n_secs: 10.0\n",
      "05:15:32 |     validation_every_n_steps: -1\n",
      "05:15:32 |     validation_max_exs: -1\n",
      "05:15:32 |     validation_metric: accuracy\n",
      "05:15:32 |     validation_metric_mode: None\n",
      "05:15:32 |     validation_patience: 10\n",
      "05:15:32 |     validation_share_agent: False\n",
      "05:15:32 |     verbose: False\n",
      "05:15:32 |     wandb_entity: None\n",
      "05:15:32 |     wandb_log: False\n",
      "05:15:32 |     wandb_name: None\n",
      "05:15:32 |     wandb_project: None\n",
      "05:15:32 |     warmup_rate: 0.0001\n",
      "05:15:32 |     warmup_updates: -1\n",
      "05:15:32 |     weight_decay: None\n",
      "05:15:33 | creating task(s): cbt\n",
      "05:15:33 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_train.txt\n",
      "05:15:40 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_train.txt\n",
      "05:15:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_train.txt\n",
      "05:16:00 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_train.txt\n",
      "05:16:25 | training...\n",
      "05:16:25 | max_train_time elapsed:3943.8686530590057s\n",
      "05:16:27 | Using CUDA\n",
      "05:16:27 | loading dictionary from my_first_lstm/model.dict\n",
      "05:16:27 | num words = 51210\n",
      "05:16:27 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "05:16:27 | Loading existing model params from my_first_lstm/model\n",
      "05:16:28 | creating task(s): cbt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_valid_2000ex.txt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_valid_2000ex.txt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_valid_2000ex.txt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_valid_2000ex.txt\n",
      "05:16:28 | running eval: valid\n",
      "05:24:48 | eval completed in 499.79s\n",
      "05:24:48 | \u001b[1mvalid:\n",
      "           accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs      f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all      .007875 7.892e-12 512.3  1536  8201       0          0 16.01 8000 .007938    .1936 2.013 9.903   1 6.035 32.22   \n",
      "   cbt:CN     .0010     1e-12 522.1                   0          0       2000   .0010          2.004 11.36                   \n",
      "   cbt:NE         0 6.767e-14   489                   0          0       2000  .00025          2.047 10.13                   \n",
      "   cbt:P      .0305  3.05e-11 528.9                   0          0       2000   .0305              2  7.11                   \n",
      "   cbt:V          0         0 509.2                   0          0       2000       0              2 11.02                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  \n",
      "   all          0          0 43190      .3975    .00775                   51 1542 8233  \n",
      "   cbt:CN       0          0 85669      .3787     .0010                                 \n",
      "   cbt:NE       0          0 25088      .3464         0                                 \n",
      "   cbt:P        0          0  1224      .4975     .0300                                 \n",
      "   cbt:V        0          0 60780      .3675         0\n",
      "\u001b[0m\n",
      "05:24:48 | creating task(s): cbt\n",
      "05:24:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_test_2500ex.txt\n",
      "05:24:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_test_2500ex.txt\n",
      "05:24:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_test_2500ex.txt\n",
      "05:24:49 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_test_2500ex.txt\n",
      "05:24:49 | running eval: test\n",
      "05:35:27 | eval completed in 638.22s\n",
      "05:35:27 | \u001b[1mtest:\n",
      "           accuracy   bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs    f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all        .0088  8.8e-12 524.4  1572  8216       0          0 15.67 10000 .0088    .1938 2.029    10   1 6.083 31.79   \n",
      "   cbt:CN         0        0 539.4                   0          0        2500     0          2.004 11.25                   \n",
      "   cbt:NE         0        0 503.7                   0          0        2500     0           2.11  10.7                   \n",
      "   cbt:P      .0352 3.52e-11 537.1                   0          0        2500 .0352              2 7.159                   \n",
      "   cbt:V          0        0 517.3                   0          0        2500     0          2.001  10.9                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  \n",
      "   all          0          0 44172      .3888     .0087                   51 1578 8248  \n",
      "   cbt:CN       0          0 76860      .3615         0                                 \n",
      "   cbt:NE       0          0 44272      .3062         0                                 \n",
      "   cbt:P        0          0  1286      .4996     .0348                                 \n",
      "   cbt:V        0          0 54270      .3880         0\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'cbt:NE/exs': SumMetric(2000),\n",
       "  'exs': SumMetric(8000),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0.00025),\n",
       "  'cbt:NE/bleu-4': BleuMetric(6.767e-14),\n",
       "  'cbt:NE/clen': AverageMetric(489),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.047),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.13),\n",
       "  'cbt:NE/ppl': PPLMetric(2.509e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.3464),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2000),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0.001),\n",
       "  'cbt:CN/f1': F1Metric(0.001),\n",
       "  'cbt:CN/bleu-4': BleuMetric(1e-12),\n",
       "  'cbt:CN/clen': AverageMetric(522.1),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.36),\n",
       "  'cbt:CN/ppl': PPLMetric(8.567e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3787),\n",
       "  'cbt:CN/token_em': AverageMetric(0.001),\n",
       "  'cbt:V/exs': SumMetric(2000),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(509.2),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(11.02),\n",
       "  'cbt:V/ppl': PPLMetric(6.078e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.3675),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2000),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0305),\n",
       "  'cbt:P/f1': F1Metric(0.0305),\n",
       "  'cbt:P/bleu-4': BleuMetric(3.05e-11),\n",
       "  'cbt:P/clen': AverageMetric(528.9),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(7.11),\n",
       "  'cbt:P/ppl': PPLMetric(1224),\n",
       "  'cbt:P/token_acc': AverageMetric(0.4975),\n",
       "  'cbt:P/token_em': AverageMetric(0.03),\n",
       "  'accuracy': MacroAverageMetric(0.007875),\n",
       "  'f1': MacroAverageMetric(0.007938),\n",
       "  'bleu-4': MacroAverageMetric(7.892e-12),\n",
       "  'clen': MacroAverageMetric(512.3),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.013),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(9.903),\n",
       "  'ppl': MacroAverageMetric(4.319e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3975),\n",
       "  'token_em': MacroAverageMetric(0.00775),\n",
       "  'exps': GlobalTimerMetric(16.01),\n",
       "  'ltpb': GlobalAverageMetric(6.035),\n",
       "  'ltps': GlobalTimerMetric(32.22),\n",
       "  'ctpb': GlobalAverageMetric(1536),\n",
       "  'ctps': GlobalTimerMetric(8201),\n",
       "  'tpb': GlobalAverageMetric(1542),\n",
       "  'tps': GlobalTimerMetric(8233),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1936),\n",
       "  'total_train_updates': GlobalFixedMetric(51)},\n",
       " {'cbt:NE/exs': SumMetric(2500),\n",
       "  'exs': SumMetric(1e+04),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0),\n",
       "  'cbt:NE/bleu-4': BleuMetric(0),\n",
       "  'cbt:NE/clen': AverageMetric(503.7),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.11),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.7),\n",
       "  'cbt:NE/ppl': PPLMetric(4.427e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.3062),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2500),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:CN/f1': F1Metric(0),\n",
       "  'cbt:CN/bleu-4': BleuMetric(0),\n",
       "  'cbt:CN/clen': AverageMetric(539.4),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.25),\n",
       "  'cbt:CN/ppl': PPLMetric(7.686e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3615),\n",
       "  'cbt:CN/token_em': AverageMetric(0),\n",
       "  'cbt:V/exs': SumMetric(2500),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(517.3),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2.001),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(10.9),\n",
       "  'cbt:V/ppl': PPLMetric(5.427e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.388),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2500),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0352),\n",
       "  'cbt:P/f1': F1Metric(0.0352),\n",
       "  'cbt:P/bleu-4': BleuMetric(3.52e-11),\n",
       "  'cbt:P/clen': AverageMetric(537.1),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(7.159),\n",
       "  'cbt:P/ppl': PPLMetric(1286),\n",
       "  'cbt:P/token_acc': AverageMetric(0.4996),\n",
       "  'cbt:P/token_em': AverageMetric(0.0348),\n",
       "  'accuracy': MacroAverageMetric(0.0088),\n",
       "  'f1': MacroAverageMetric(0.0088),\n",
       "  'bleu-4': MacroAverageMetric(8.8e-12),\n",
       "  'clen': MacroAverageMetric(524.4),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.029),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(10),\n",
       "  'ppl': MacroAverageMetric(4.417e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3888),\n",
       "  'token_em': MacroAverageMetric(0.0087),\n",
       "  'exps': GlobalTimerMetric(15.67),\n",
       "  'ltpb': GlobalAverageMetric(6.083),\n",
       "  'ltps': GlobalTimerMetric(31.79),\n",
       "  'ctpb': GlobalAverageMetric(1572),\n",
       "  'ctps': GlobalTimerMetric(8216),\n",
       "  'tpb': GlobalAverageMetric(1578),\n",
       "  'tps': GlobalTimerMetric(8248),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1938),\n",
       "  'total_train_updates': GlobalFixedMetric(51)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainModel.main(\n",
    "    model='my_first_lstm',\n",
    "    model_file='my_first_lstm/model',\n",
    "    task='cbt',\n",
    "    # Entrenamos el modelo con un lote\n",
    "    batchsize=3,\n",
    "    validation_every_n_secs=10,\n",
    "    max_train_time=60,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TDJFWJk2JL_C",
    "cBfO781TltNR"
   ],
   "name": "PC5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
