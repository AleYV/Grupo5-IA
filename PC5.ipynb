{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gphowBNtIwfi"
   },
   "source": [
    "# ParlIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDJFWJk2JL_C"
   },
   "source": [
    "## Instalación del ParlIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FP5uSxThJJ0p"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q parlai\n",
    "!pip3 install -q subword_nmt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOs6sfkqKpnB"
   },
   "source": [
    "## Importando el Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIKL-D12bdtd"
   },
   "source": [
    "### Importamos el Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1eQSLdVDbiZU"
   },
   "outputs": [],
   "source": [
    "from parlai.core.build_data import DownloadableFile\n",
    "import parlai.core.build_data as build_data\n",
    "import os\n",
    "\n",
    "RESOURCES = [\n",
    "    DownloadableFile(\n",
    "        'http://parl.ai/downloads/cbt/cbt.tar.gz',\n",
    "        'cbt.tar.gz',\n",
    "        '932df0cadc1337b2a12b4c696b1041c1d1c6d4b6bd319874c6288f02e4a61e92',\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "def build(opt):\n",
    "    dpath = os.path.join(opt['datapath'], 'CBT')\n",
    "    version = None\n",
    "\n",
    "    if not build_data.built(dpath, version_string=version):\n",
    "        print('[building data: ' + dpath + ']')\n",
    "        if build_data.built(dpath):\n",
    "            # Se elimina los archivos desactualizados de la versión anterior.\n",
    "            build_data.remove_dir(dpath)\n",
    "        build_data.make_dir(dpath)\n",
    "\n",
    "        # Descarga los datos.\n",
    "        for downloadable_file in RESOURCES:\n",
    "            downloadable_file.download_file(dpath)\n",
    "\n",
    "        # Marca los datos como construidos.\n",
    "        build_data.mark_done(dpath, version_string=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostramos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:20:10 | Opt:\n",
      "06:20:10 |     allow_missing_init_opts: False\n",
      "06:20:10 |     batchsize: 1\n",
      "06:20:10 |     datapath: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\n",
      "06:20:10 |     datatype: train:ordered\n",
      "06:20:10 |     dict_class: None\n",
      "06:20:10 |     display_add_fields: \n",
      "06:20:10 |     download_path: None\n",
      "06:20:10 |     dynamic_batching: None\n",
      "06:20:10 |     hide_labels: False\n",
      "06:20:10 |     ignore_agent_reply: True\n",
      "06:20:10 |     image_cropsize: 224\n",
      "06:20:10 |     image_mode: raw\n",
      "06:20:10 |     image_size: 256\n",
      "06:20:10 |     init_model: None\n",
      "06:20:10 |     init_opt: None\n",
      "06:20:10 |     is_debug: False\n",
      "06:20:10 |     loglevel: info\n",
      "06:20:10 |     max_display_len: 1000\n",
      "06:20:10 |     model: None\n",
      "06:20:10 |     model_file: None\n",
      "06:20:10 |     multitask_weights: [1]\n",
      "06:20:10 |     mutators: None\n",
      "06:20:10 |     num_examples: 1\n",
      "06:20:10 |     override: \"{'task': 'cbt', 'num_examples': 1}\"\n",
      "06:20:10 |     parlai_home: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\n",
      "06:20:10 |     starttime: Dec05_06-20\n",
      "06:20:10 |     task: cbt\n",
      "06:20:10 |     verbose: False\n",
      "06:20:11 | creating task(s): cbt\n",
      "06:20:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_train.txt\n",
      "06:20:18 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_train.txt\n",
      "06:21:01 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_train.txt\n",
      "06:21:08 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_train.txt\n",
      "\u001b[1;31m- - - NEW EPISODE: cbt:NE - - -\u001b[0;0m\n",
      "\u001b[0mFill in the blank in the last sentence.\n",
      "Some were abroad ; several were ill ; a few were in prison among the Saracens ; others were captives in the dens of ogres .\n",
      "The end of it was that the king and queen had to sit down alone , one at each end of a very long table , arrayed with plates and glasses for a hundred guests -- for a hundred guests who never came !\n",
      "`` Any soup , my dear ? ''\n",
      "shouted the king , through a speaking-trumpet ; when , suddenly , the air was filled with a sound like the rustling of the wings of birds .\n",
      "Flitter , flitter , flutter , went the noise ; and when the queen looked up , lo and behold !\n",
      "on every seat was a lovely fairy , dressed in green , each with a most interesting-looking parcel in her hand .\n",
      "Do n't you like opening parcels ?\n",
      "The king did , and he was most friendly and polite to the fairies .\n",
      "But the queen , though she saw them distinctly , took no notice of them .\n",
      "You see , she did not believe in fairies , nor in her own eyes , when she saw them .\n",
      "So she talked across the fairies to the king , just as if they had not been there ; but the king behaved as politely as if they were real -- which , of course , they were .\n",
      "When dinner was over , and when the nurse had brought in the baby , all the fairies gave him the most magnificent presents .\n",
      "One offered a purse which could never be empty ; and one a pair of seven-leagued boots ; and another a cap of darkness , that nobody might see the prince when he put it on ; and another a wishing-cap ; and another a carpet , on which , when he sat , he was carried wherever he wished to find himself .\n",
      "Another made him beautiful for ever ; and another , brave ; and another , lucky : but the last fairy of all , a cross old thing , crept up and said , `` My child , you shall be too clever ! ''\n",
      "This fairy 's gift would have pleased the queen , if she had believed in it , more than anything else , because she was so clever herself .\n",
      "But she took no notice at all ; and the fairies went each to her own country , and none of them stayed there at the palace , where nobody believed in them , except the king , a little .\n",
      "But the queen tossed all their nice boots and caps , carpets , purses , swords , and all , away into a dark lumber-room ; for , of course , she thought that they were all nonsense , and merely old rubbish out of books , or pantomime `` properties . ''\n",
      "CHAPTER II .\n",
      "-LCB- Chapter heading picture : p9.jpg -RCB- Prince Prigio and his Family .\n",
      "Well , the little prince grew up .\n",
      "I think I 've told you that his name was XXXXX -- did I not ?\u001b[0;0m\n",
      "   \u001b[1;94mPrigio\u001b[0;0m\n",
      "06:21:34 | loaded 669343 episodes with a total of 669343 examples\n"
     ]
    }
   ],
   "source": [
    "# El script display_data se usa para mostrar el contenido de una tarea en particular.\n",
    "# Por defecto, mostramos el train\n",
    "from parlai.scripts.display_data import DisplayData\n",
    "DisplayData.main(task='cbt', num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tqVe18TcRi7"
   },
   "source": [
    "### Implementamos los agentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r2J9wku5cT1U"
   },
   "outputs": [],
   "source": [
    "from parlai.core.teachers import FbDeprecatedDialogTeacher, MultiTaskTeacher\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "\n",
    "def _path(task, opt):\n",
    "    # Genera los datos si no existen.\n",
    "    build(opt)\n",
    "    suffix = ''\n",
    "    dt = opt['datatype'].split(':')[0]\n",
    "    if dt == 'train':\n",
    "        suffix = 'train'\n",
    "    elif dt == 'test':\n",
    "        suffix = 'test_2500ex'\n",
    "    elif dt == 'valid':\n",
    "        suffix = 'valid_2000ex'\n",
    "\n",
    "    return os.path.join(\n",
    "        opt['datapath'], 'CBT', 'CBTest', 'data', task + '_' + suffix + '.txt'\n",
    "    )\n",
    "\n",
    "\n",
    "class NETeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_NE', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "class CNTeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_CN', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "class VTeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_V', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "class PTeacher(FbDeprecatedDialogTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt['datafile'] = _path('cbtest_P', opt)\n",
    "        opt['cloze'] = True\n",
    "        super().__init__(opt, shared)\n",
    "\n",
    "\n",
    "# De forma predeterminada, entrena en todas las tareas a la vez.\n",
    "\n",
    "class DefaultTeacher(MultiTaskTeacher):\n",
    "    def __init__(self, opt, shared=None):\n",
    "        opt = copy.deepcopy(opt)\n",
    "        opt['task'] = 'cbt:NE,cbt:CN,cbt:V,cbt:P'\n",
    "        super().__init__(opt, shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSPIjinOkZdL"
   },
   "source": [
    "### Anadir el task al ParlAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uaXkaJKikddp"
   },
   "outputs": [],
   "source": [
    "from parlai.utils.testing import AutoTeacherTest  \n",
    "\n",
    "class TestDefaultTeacher(AutoTeacherTest):\n",
    "    task = 'cbt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBfO781TltNR"
   },
   "source": [
    "# Creando nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hVrZh-T903wh"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import parlai.core.torch_generator_agent as tga\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Codificador de ejemplo, que consta de una capa de incrustación y un LSTM de 1 capa con el\n",
    "    tamaño oculto especificado.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size):\n",
    "        \"\"\"\n",
    "        Inicialización.\n",
    "        Los argumentos aquí se pueden utilizar para proporcionar hiperparámetros.\n",
    "        \"\"\"\n",
    "        # Llama a super en todos los nn.Modules para que lo herede.\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        \"\"\"            \n",
    "        Realice el forward pass para el codificador.\n",
    "        La entrada debe ser input_tokens, que son los tokens de contexto dados\n",
    "        como una matriz de ID de búsqueda.\n",
    "        : param input_tokens:\n",
    "            Introduzca tokens como bsz x seqlen LongTensor.\n",
    "            Probablemente contendrá relleno.\n",
    "        :regreso:\n",
    "            Puede devolver lo que quiera; se pasará palabra por palabra\n",
    "            en el decodificador para acondicionamiento. Sin embargo, debería ser algo\n",
    "            puede manipular fácilmente en '' reorder_encoder_states ''.\n",
    "            Esta implementación particular devuelve los estados ocultos y de celda de la\n",
    "            LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embeddings(input_tokens)\n",
    "        _output, hidden = self.lstm(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodificador de ejemplo básico, que consta de una capa de incrustación y un LSTM de 1 capa con el\n",
    "    tamaño oculto especificado. El decodificador permite la decodificación incremental ingiriendo el\n",
    "    estado incremental actual en cada pasada hacia adelante.\n",
    "    Preste especial atención al \"forward\".\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings, hidden_size):\n",
    "        \"\"\"\n",
    "        Inicialización.\n",
    "        Los argumentos aquí se pueden utilizar para proporcionar hiperparámetros.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input, encoder_state, incr_state=None):\n",
    "        \"\"\"\n",
    "        Correr el forward pass.\n",
    "        :parámetro input:\n",
    "            Los tokens generados actualmente por el decodificador.\n",
    "        :parámetro encoder_state:\n",
    "            La salida del módulo codificador.\n",
    "        :parámetro incr_state:\n",
    "            El estado oculto anterior del decodificador. \n",
    "        \"\"\"\n",
    "        embedded = self.embeddings(input)\n",
    "        if incr_state is None:\n",
    "            # esta es nuestra primera llamada. Queremos sembrar el LSTM con el estado oculto del decodificador.\n",
    "            state = encoder_state\n",
    "        else:\n",
    "            # Ya hemos generado algunos tokens, por lo que podemos reutilizar el estado del decodificador existente\n",
    "            state = incr_state\n",
    "\n",
    "        # obtener la nueva salida y el estado incremental del decodificador\n",
    "        output, incr_state = self.lstm(embedded, state)\n",
    "\n",
    "        return output, incr_state\n",
    "\n",
    "\n",
    "class ExampleModel(tga.TorchGeneratorModel):\n",
    "    \"\"\"\n",
    "    ExampleModel implementa los métodos abstractos de TorchGeneratorModel para definir cómo\n",
    "    reordenar los estados del codificador y los estados incrementales del descodificador.\n",
    "    También crea una instancia de la tabla de incrustación, el codificador y el descodificador, y define el\n",
    "    capa de salida final.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dictionary, hidden_size=1024):\n",
    "        super().__init__(\n",
    "            padding_idx=dictionary[dictionary.null_token],\n",
    "            start_idx=dictionary[dictionary.start_token],\n",
    "            end_idx=dictionary[dictionary.end_token],\n",
    "            unknown_idx=dictionary[dictionary.unk_token],\n",
    "        )\n",
    "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
    "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
    "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
    "\n",
    "    def output(self, decoder_output):\n",
    "        \"\"\"\n",
    "        Realiza la salida final -> transformación logits.\n",
    "        \"\"\"\n",
    "        return F.linear(decoder_output, self.embeddings.weight)\n",
    "\n",
    "    def reorder_encoder_states(self, encoder_states, indices):\n",
    "        \"\"\"\n",
    "        Reordena los estados del codificador para seleccionar solo los índices de lote dados.\n",
    "        Se indexa la selección en la dimensión del lote.\n",
    "        \"\"\"\n",
    "        h, c = encoder_states\n",
    "        return h[:, indices, :], c[:, indices, :]\n",
    "\n",
    "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
    "        \"\"\"\n",
    "        Este método puede ser un código auxiliar que siempre devuelve None; esto resultará en el\n",
    "        decodificador haciendo un pase completo hacia adelante para cada token.\n",
    "        Sin embargo, si cualquier estado se puede almacenar en caché, este método debe ser\n",
    "        implementado para reducir la complejidad de generación.\n",
    "        \"\"\"\n",
    "        h, c = incr_state\n",
    "        return h[:, indices, :], c[:, indices, :]\n",
    "\n",
    "\n",
    "@register_agent(\"my_first_lstm\")\n",
    "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
    "    \"\"\"\n",
    "    Implementa la interfaz para TorchGeneratorAgent.\n",
    "    Implementa `` build_model '', se incluye una línea de comando adicional\n",
    "    parámetros.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def add_cmdline_args(cls, argparser, partial_opt):\n",
    "        \"\"\"       \n",
    "        Agregar argumentos CLI.\n",
    "        \"\"\"\n",
    "        # Agrega todos los argumentos de TorchGeneratorAgent\n",
    "        super().add_cmdline_args(argparser)\n",
    "\n",
    "        # Agregamos argumentos personalizados solo para este modelo.\n",
    "        group = argparser.add_argument_group('Example TGA Agent')\n",
    "        group.add_argument(\n",
    "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
    "        )\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"  \n",
    "        Construye el modelo.\n",
    "        \"\"\"\n",
    "\n",
    "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
    "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMQ4N0Bfm67-"
   },
   "source": [
    "## Entrenando el Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lfjG72qcnGch",
    "outputId": "66dab851-05e3-433e-d8b4-7d2287b8f7c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:49:52 | building dictionary first...\n",
      "03:49:52 | No model with opt yet at: my_first_lstm/model(.opt)\n",
      "03:49:52 | Using CUDA\n",
      "03:49:52 | loading dictionary from my_first_lstm/model.dict\n",
      "03:49:52 | num words = 51210\n",
      "03:49:52 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "03:49:52 | Opt:\n",
      "03:49:52 |     adafactor_eps: '(1e-30, 0.001)'\n",
      "03:49:52 |     adam_eps: 1e-08\n",
      "03:49:52 |     add_p1_after_newln: False\n",
      "03:49:52 |     aggregate_micro: False\n",
      "03:49:52 |     allow_missing_init_opts: False\n",
      "03:49:52 |     batchsize: 1\n",
      "03:49:52 |     beam_block_full_context: True\n",
      "03:49:52 |     beam_block_list_filename: None\n",
      "03:49:52 |     beam_block_ngram: -1\n",
      "03:49:52 |     beam_context_block_ngram: -1\n",
      "03:49:52 |     beam_delay: 30\n",
      "03:49:52 |     beam_length_penalty: 0.65\n",
      "03:49:52 |     beam_min_length: 1\n",
      "03:49:52 |     beam_size: 1\n",
      "03:49:52 |     betas: '(0.9, 0.999)'\n",
      "03:49:52 |     bpe_add_prefix_space: None\n",
      "03:49:52 |     bpe_debug: False\n",
      "03:49:52 |     bpe_dropout: None\n",
      "03:49:52 |     bpe_merge: None\n",
      "03:49:52 |     bpe_vocab: None\n",
      "03:49:52 |     compute_tokenized_bleu: False\n",
      "03:49:52 |     datapath: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\n",
      "03:49:52 |     datatype: train\n",
      "03:49:52 |     delimiter: '\\n'\n",
      "03:49:52 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "03:49:52 |     dict_endtoken: __end__\n",
      "03:49:52 |     dict_file: my_first_lstm/model.dict\n",
      "03:49:52 |     dict_include_test: False\n",
      "03:49:52 |     dict_include_valid: False\n",
      "03:49:52 |     dict_initpath: None\n",
      "03:49:52 |     dict_language: english\n",
      "03:49:52 |     dict_loaded: True\n",
      "03:49:52 |     dict_lower: False\n",
      "03:49:52 |     dict_max_ngram_size: -1\n",
      "03:49:52 |     dict_maxexs: -1\n",
      "03:49:52 |     dict_maxtokens: -1\n",
      "03:49:52 |     dict_minfreq: 0\n",
      "03:49:52 |     dict_nulltoken: __null__\n",
      "03:49:52 |     dict_starttoken: __start__\n",
      "03:49:52 |     dict_textfields: text,labels\n",
      "03:49:52 |     dict_tokenizer: re\n",
      "03:49:52 |     dict_unktoken: __unk__\n",
      "03:49:52 |     display_examples: False\n",
      "03:49:52 |     download_path: None\n",
      "03:49:52 |     dynamic_batching: None\n",
      "03:49:52 |     embedding_projection: random\n",
      "03:49:52 |     embedding_type: random\n",
      "03:49:52 |     eval_batchsize: None\n",
      "03:49:52 |     eval_dynamic_batching: None\n",
      "03:49:52 |     evaltask: None\n",
      "03:49:52 |     final_extra_opt: \n",
      "03:49:52 |     force_fp16_tokens: False\n",
      "03:49:52 |     fp16: False\n",
      "03:49:52 |     fp16_impl: safe\n",
      "03:49:52 |     gpu: -1\n",
      "03:49:52 |     gradient_clip: 0.1\n",
      "03:49:52 |     hidden_size: 1024\n",
      "03:49:52 |     hide_labels: False\n",
      "03:49:52 |     history_add_global_end_token: None\n",
      "03:49:52 |     history_reversed: False\n",
      "03:49:52 |     history_size: -1\n",
      "03:49:52 |     image_cropsize: 224\n",
      "03:49:52 |     image_mode: raw\n",
      "03:49:52 |     image_size: 256\n",
      "03:49:52 |     inference: greedy\n",
      "03:49:52 |     init_model: None\n",
      "03:49:52 |     init_opt: None\n",
      "03:49:52 |     interactive_mode: False\n",
      "03:49:52 |     invsqrt_lr_decay_gamma: -1\n",
      "03:49:52 |     is_debug: False\n",
      "03:49:52 |     label_truncate: None\n",
      "03:49:52 |     learningrate: 1\n",
      "03:49:52 |     load_from_checkpoint: True\n",
      "03:49:52 |     log_every_n_secs: -1\n",
      "03:49:52 |     log_every_n_steps: 50\n",
      "03:49:52 |     loglevel: info\n",
      "03:49:52 |     lr_scheduler: reduceonplateau\n",
      "03:49:52 |     lr_scheduler_decay: 0.5\n",
      "03:49:52 |     lr_scheduler_patience: 3\n",
      "03:49:52 |     max_train_steps: -1\n",
      "03:49:52 |     max_train_time: 60.0\n",
      "03:49:52 |     metrics: default\n",
      "03:49:52 |     model: my_first_lstm\n",
      "03:49:52 |     model_file: my_first_lstm/model\n",
      "03:49:52 |     momentum: 0\n",
      "03:49:52 |     multitask_weights: [1]\n",
      "03:49:52 |     mutators: None\n",
      "03:49:52 |     nesterov: True\n",
      "03:49:52 |     no_cuda: False\n",
      "03:49:52 |     num_epochs: -1\n",
      "03:49:52 |     num_workers: 0\n",
      "03:49:52 |     nus: (0.7,)\n",
      "03:49:52 |     optimizer: sgd\n",
      "03:49:52 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'cbt', 'batchsize': 1, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
      "03:49:52 |     parlai_home: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\n",
      "03:49:52 |     person_tokens: False\n",
      "03:49:52 |     rank_candidates: False\n",
      "03:49:52 |     save_after_valid: False\n",
      "03:49:52 |     save_every_n_secs: -1\n",
      "03:49:52 |     short_final_eval: False\n",
      "03:49:52 |     skip_generation: False\n",
      "03:49:52 |     special_tok_lst: None\n",
      "03:49:52 |     split_lines: False\n",
      "03:49:52 |     starttime: Dec05_03-49\n",
      "03:49:52 |     task: cbt\n",
      "03:49:52 |     temperature: 1.0\n",
      "03:49:52 |     tensorboard_log: False\n",
      "03:49:52 |     tensorboard_logdir: None\n",
      "03:49:52 |     text_truncate: None\n",
      "03:49:52 |     topk: 10\n",
      "03:49:52 |     topp: 0.9\n",
      "03:49:52 |     truncate: -1\n",
      "03:49:52 |     update_freq: 1\n",
      "03:49:52 |     use_reply: label\n",
      "03:49:52 |     validation_cutoff: 1.0\n",
      "03:49:53 |     validation_every_n_epochs: -1\n",
      "03:49:53 |     validation_every_n_secs: 10.0\n",
      "03:49:53 |     validation_every_n_steps: -1\n",
      "03:49:53 |     validation_max_exs: -1\n",
      "03:49:53 |     validation_metric: accuracy\n",
      "03:49:53 |     validation_metric_mode: None\n",
      "03:49:53 |     validation_patience: 10\n",
      "03:49:53 |     validation_share_agent: False\n",
      "03:49:53 |     verbose: False\n",
      "03:49:53 |     wandb_entity: None\n",
      "03:49:53 |     wandb_log: False\n",
      "03:49:53 |     wandb_name: None\n",
      "03:49:53 |     wandb_project: None\n",
      "03:49:53 |     warmup_rate: 0.0001\n",
      "03:49:53 |     warmup_updates: -1\n",
      "03:49:53 |     weight_decay: None\n",
      "03:49:53 | creating task(s): cbt\n",
      "03:49:53 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_train.txt\n",
      "03:49:59 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_train.txt\n",
      "03:50:09 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_train.txt\n",
      "03:50:15 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_train.txt\n",
      "03:50:41 | training...\n",
      "03:50:52 | time:10s total_exs:50 total_steps:50 epochs:0.00\n",
      "           clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  \\\n",
      "   all    563.1     1   557  2729       0          0 4.899   50  175.8    .4118     2 12.54   1     2 9.799       0   \n",
      "   cbt:CN 602.9                         0          0         10                     2 13.46                       0   \n",
      "   cbt:NE 573.8                         0          0          5                     2 15.46                       0   \n",
      "   cbt:P  580.1                         0          0         19                     2 9.126                       0   \n",
      "   cbt:V  495.6                         0          0         16                     2 12.11                       0   \n",
      "           ltrunclen     ppl  token_acc  token_em  total_train_updates  tpb  tps   ups  \n",
      "   all             0 1513935      .2605         0                   50  559 2739 4.899  \n",
      "   cbt:CN          0  701097      .2500         0                                       \n",
      "   cbt:NE          0 5163857      .2000         0                                       \n",
      "   cbt:P           0    9188      .3421         0                                       \n",
      "   cbt:V           0  181597      .2500         0\n",
      "\n",
      "03:50:52 | creating task(s): cbt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_valid_2000ex.txt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_valid_2000ex.txt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_valid_2000ex.txt\n",
      "03:50:52 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_valid_2000ex.txt\n",
      "03:50:52 | running eval: valid\n",
      "04:11:08 | eval completed in 1216.06s\n",
      "04:11:08 | \u001b[1mvalid:\n",
      "           accuracy   bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs      f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all      .000625 6.25e-13 512.3 512.3  3370       0          0 6.579 8000 .000625    .2580 2.013 9.236   1 2.013 13.24   \n",
      "   cbt:CN     .0010    1e-12 522.1                   0          0       2000   .0010          2.004 11.07                   \n",
      "   cbt:NE         0        0   489                   0          0       2000       0          2.047 10.06                   \n",
      "   cbt:P      .0015  1.5e-12 528.9                   0          0       2000   .0015              2 5.784                   \n",
      "   cbt:V          0        0 509.2                   0          0       2000       0              2 10.04                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "   all          0          0 27608      .3525   .000625                   50 514.3 3384  \n",
      "   cbt:CN       0          0 63955      .3403     .0010                                  \n",
      "   cbt:NE       0          0 23331      .2694         0                                  \n",
      "   cbt:P        0          0   325      .4740     .0015                                  \n",
      "   cbt:V        0          0 22820      .3262         0\n",
      "\u001b[0m\n",
      "04:11:08 | \u001b[1;32mnew best accuracy: 0.000625\u001b[0m\n",
      "04:11:08 | saving best valid model: my_first_lstm/model\n",
      "04:11:09 | max_train_time elapsed:1227.7263507843018s\n",
      "04:11:10 | Using CUDA\n",
      "04:11:10 | loading dictionary from my_first_lstm/model.dict\n",
      "04:11:10 | num words = 51210\n",
      "04:11:11 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "04:11:11 | Loading existing model params from my_first_lstm/model\n",
      "04:11:11 | creating task(s): cbt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_valid_2000ex.txt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_valid_2000ex.txt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_valid_2000ex.txt\n",
      "04:11:11 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_valid_2000ex.txt\n",
      "04:11:12 | running eval: valid\n",
      "04:31:07 | eval completed in 1195.34s\n",
      "04:31:07 | \u001b[1mvalid:\n",
      "           accuracy   bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs      f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all      .000625 6.25e-13 512.3 512.3  3429       0          0 6.693 8000 .000625    .1936 2.013 9.236   1 2.013 13.47   \n",
      "   cbt:CN     .0010    1e-12 522.1                   0          0       2000   .0010          2.004 11.07                   \n",
      "   cbt:NE         0        0   489                   0          0       2000       0          2.047 10.06                   \n",
      "   cbt:P      .0015  1.5e-12 528.9                   0          0       2000   .0015              2 5.784                   \n",
      "   cbt:V          0        0 509.2                   0          0       2000       0              2 10.04                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "   all          0          0 27608      .3525   .000625                   50 514.3 3442  \n",
      "   cbt:CN       0          0 63955      .3403     .0010                                  \n",
      "   cbt:NE       0          0 23331      .2694         0                                  \n",
      "   cbt:P        0          0   325      .4740     .0015                                  \n",
      "   cbt:V        0          0 22820      .3262         0\n",
      "\u001b[0m\n",
      "04:31:07 | creating task(s): cbt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_test_2500ex.txt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_test_2500ex.txt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_test_2500ex.txt\n",
      "04:31:07 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_test_2500ex.txt\n",
      "04:31:08 | running eval: test\n",
      "04:56:25 | eval completed in 1517.25s\n",
      "04:56:25 | \u001b[1mtest:\n",
      "           accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs       f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all        .0003 3.368e-13 524.4 524.4  3456       0          0 6.591 10000 .0003667    .1937 2.029 9.459   1 2.029 13.37   \n",
      "   cbt:CN     .0008     8e-13 539.3                   0          0        2500    .0008          2.004 11.03                   \n",
      "   cbt:NE         0 1.472e-13 503.7                   0          0        2500 .0002667           2.11 10.97                   \n",
      "   cbt:P      .0004     4e-13 537.1                   0          0        2500    .0004              2  5.86                   \n",
      "   cbt:V          0         0 517.3                   0          0        2500        0          2.001 9.969                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates   tpb  tps  \n",
      "   all          0          0 35458      .3458     .0003                   50 526.4 3469  \n",
      "   cbt:CN       0          0 61960      .3154     .0008                                  \n",
      "   cbt:NE       0          0 58171      .2527         0                                  \n",
      "   cbt:P        0          0 350.8      .4698     .0004                                  \n",
      "   cbt:V        0          0 21349      .3455         0\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'cbt:NE/exs': SumMetric(2000),\n",
       "  'exs': SumMetric(8000),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0),\n",
       "  'cbt:NE/bleu-4': BleuMetric(0),\n",
       "  'cbt:NE/clen': AverageMetric(489),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.047),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.06),\n",
       "  'cbt:NE/ppl': PPLMetric(2.333e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.2694),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2000),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0.001),\n",
       "  'cbt:CN/f1': F1Metric(0.001),\n",
       "  'cbt:CN/bleu-4': BleuMetric(1e-12),\n",
       "  'cbt:CN/clen': AverageMetric(522.1),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.07),\n",
       "  'cbt:CN/ppl': PPLMetric(6.395e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3403),\n",
       "  'cbt:CN/token_em': AverageMetric(0.001),\n",
       "  'cbt:V/exs': SumMetric(2000),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(509.2),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(10.04),\n",
       "  'cbt:V/ppl': PPLMetric(2.282e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.3262),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2000),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0015),\n",
       "  'cbt:P/f1': F1Metric(0.0015),\n",
       "  'cbt:P/bleu-4': BleuMetric(1.5e-12),\n",
       "  'cbt:P/clen': AverageMetric(528.9),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(5.784),\n",
       "  'cbt:P/ppl': PPLMetric(325),\n",
       "  'cbt:P/token_acc': AverageMetric(0.474),\n",
       "  'cbt:P/token_em': AverageMetric(0.0015),\n",
       "  'accuracy': MacroAverageMetric(0.000625),\n",
       "  'f1': MacroAverageMetric(0.000625),\n",
       "  'bleu-4': MacroAverageMetric(6.25e-13),\n",
       "  'clen': MacroAverageMetric(512.3),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.013),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(9.236),\n",
       "  'ppl': MacroAverageMetric(2.761e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3525),\n",
       "  'token_em': MacroAverageMetric(0.000625),\n",
       "  'exps': GlobalTimerMetric(6.693),\n",
       "  'ltpb': GlobalAverageMetric(2.013),\n",
       "  'ltps': GlobalTimerMetric(13.47),\n",
       "  'ctpb': GlobalAverageMetric(512.3),\n",
       "  'ctps': GlobalTimerMetric(3429),\n",
       "  'tpb': GlobalAverageMetric(514.3),\n",
       "  'tps': GlobalTimerMetric(3442),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1936),\n",
       "  'total_train_updates': GlobalFixedMetric(50)},\n",
       " {'cbt:NE/exs': SumMetric(2500),\n",
       "  'exs': SumMetric(1e+04),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0.0002667),\n",
       "  'cbt:NE/bleu-4': BleuMetric(1.472e-13),\n",
       "  'cbt:NE/clen': AverageMetric(503.7),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.11),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.97),\n",
       "  'cbt:NE/ppl': PPLMetric(5.817e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.2527),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2500),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0.0008),\n",
       "  'cbt:CN/f1': F1Metric(0.0008),\n",
       "  'cbt:CN/bleu-4': BleuMetric(8e-13),\n",
       "  'cbt:CN/clen': AverageMetric(539.3),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.03),\n",
       "  'cbt:CN/ppl': PPLMetric(6.196e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3154),\n",
       "  'cbt:CN/token_em': AverageMetric(0.0008),\n",
       "  'cbt:V/exs': SumMetric(2500),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(517.3),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2.001),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(9.969),\n",
       "  'cbt:V/ppl': PPLMetric(2.135e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.3455),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2500),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0004),\n",
       "  'cbt:P/f1': F1Metric(0.0004),\n",
       "  'cbt:P/bleu-4': BleuMetric(4e-13),\n",
       "  'cbt:P/clen': AverageMetric(537.1),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(5.86),\n",
       "  'cbt:P/ppl': PPLMetric(350.8),\n",
       "  'cbt:P/token_acc': AverageMetric(0.4698),\n",
       "  'cbt:P/token_em': AverageMetric(0.0004),\n",
       "  'accuracy': MacroAverageMetric(0.0003),\n",
       "  'f1': MacroAverageMetric(0.0003667),\n",
       "  'bleu-4': MacroAverageMetric(3.368e-13),\n",
       "  'clen': MacroAverageMetric(524.4),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.029),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(9.459),\n",
       "  'ppl': MacroAverageMetric(3.546e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3458),\n",
       "  'token_em': MacroAverageMetric(0.0003),\n",
       "  'exps': GlobalTimerMetric(6.591),\n",
       "  'ltpb': GlobalAverageMetric(2.029),\n",
       "  'ltps': GlobalTimerMetric(13.37),\n",
       "  'ctpb': GlobalAverageMetric(524.4),\n",
       "  'ctps': GlobalTimerMetric(3456),\n",
       "  'tpb': GlobalAverageMetric(526.4),\n",
       "  'tps': GlobalTimerMetric(3469),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1937),\n",
       "  'total_train_updates': GlobalFixedMetric(50)})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parlai.scripts.train_model import TrainModel\n",
    "from parlai.core.agents import create_agent\n",
    "\n",
    "TrainModel.main(\n",
    "    model='my_first_lstm',\n",
    "    model_file='my_first_lstm/model',\n",
    "    task='cbt',\n",
    "    batchsize=1,\n",
    "    validation_every_n_secs=10,\n",
    "    max_train_time=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05:15:31 | building dictionary first...\n",
      "05:15:31 | \u001b[33mOverriding opt[\"batchsize\"] to 3 (previously: 1)\u001b[0m\n",
      "05:15:31 | Using CUDA\n",
      "05:15:31 | loading dictionary from my_first_lstm/model.dict\n",
      "05:15:31 | num words = 51210\n",
      "05:15:32 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "05:15:32 | Loading existing model params from my_first_lstm/model\n",
      "05:15:32 | Opt:\n",
      "05:15:32 |     adafactor_eps: '[1e-30, 0.001]'\n",
      "05:15:32 |     adam_eps: 1e-08\n",
      "05:15:32 |     add_p1_after_newln: False\n",
      "05:15:32 |     aggregate_micro: False\n",
      "05:15:32 |     allow_missing_init_opts: False\n",
      "05:15:32 |     batchsize: 3\n",
      "05:15:32 |     beam_block_full_context: True\n",
      "05:15:32 |     beam_block_list_filename: None\n",
      "05:15:32 |     beam_block_ngram: -1\n",
      "05:15:32 |     beam_context_block_ngram: -1\n",
      "05:15:32 |     beam_delay: 30\n",
      "05:15:32 |     beam_length_penalty: 0.65\n",
      "05:15:32 |     beam_min_length: 1\n",
      "05:15:32 |     beam_size: 1\n",
      "05:15:32 |     betas: '[0.9, 0.999]'\n",
      "05:15:32 |     bpe_add_prefix_space: None\n",
      "05:15:32 |     bpe_debug: False\n",
      "05:15:32 |     bpe_dropout: None\n",
      "05:15:32 |     bpe_merge: None\n",
      "05:15:32 |     bpe_vocab: None\n",
      "05:15:32 |     compute_tokenized_bleu: False\n",
      "05:15:32 |     datapath: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\n",
      "05:15:32 |     datatype: train\n",
      "05:15:32 |     delimiter: '\\n'\n",
      "05:15:32 |     dict_class: parlai.core.dict:DictionaryAgent\n",
      "05:15:32 |     dict_endtoken: __end__\n",
      "05:15:32 |     dict_file: my_first_lstm/model.dict\n",
      "05:15:32 |     dict_include_test: False\n",
      "05:15:32 |     dict_include_valid: False\n",
      "05:15:32 |     dict_initpath: None\n",
      "05:15:32 |     dict_language: english\n",
      "05:15:32 |     dict_loaded: True\n",
      "05:15:32 |     dict_lower: False\n",
      "05:15:32 |     dict_max_ngram_size: -1\n",
      "05:15:32 |     dict_maxexs: -1\n",
      "05:15:32 |     dict_maxtokens: -1\n",
      "05:15:32 |     dict_minfreq: 0\n",
      "05:15:32 |     dict_nulltoken: __null__\n",
      "05:15:32 |     dict_starttoken: __start__\n",
      "05:15:32 |     dict_textfields: text,labels\n",
      "05:15:32 |     dict_tokenizer: re\n",
      "05:15:32 |     dict_unktoken: __unk__\n",
      "05:15:32 |     display_examples: False\n",
      "05:15:32 |     download_path: None\n",
      "05:15:32 |     dynamic_batching: None\n",
      "05:15:32 |     embedding_projection: random\n",
      "05:15:32 |     embedding_type: random\n",
      "05:15:32 |     eval_batchsize: None\n",
      "05:15:32 |     eval_dynamic_batching: None\n",
      "05:15:32 |     evaltask: None\n",
      "05:15:32 |     final_extra_opt: \n",
      "05:15:32 |     force_fp16_tokens: False\n",
      "05:15:32 |     fp16: False\n",
      "05:15:32 |     fp16_impl: safe\n",
      "05:15:32 |     gpu: -1\n",
      "05:15:32 |     gradient_clip: 0.1\n",
      "05:15:32 |     hidden_size: 1024\n",
      "05:15:32 |     hide_labels: False\n",
      "05:15:32 |     history_add_global_end_token: None\n",
      "05:15:32 |     history_reversed: False\n",
      "05:15:32 |     history_size: -1\n",
      "05:15:32 |     image_cropsize: 224\n",
      "05:15:32 |     image_mode: raw\n",
      "05:15:32 |     image_size: 256\n",
      "05:15:32 |     inference: greedy\n",
      "05:15:32 |     init_model: None\n",
      "05:15:32 |     init_opt: None\n",
      "05:15:32 |     interactive_mode: False\n",
      "05:15:32 |     invsqrt_lr_decay_gamma: -1\n",
      "05:15:32 |     is_debug: False\n",
      "05:15:32 |     label_truncate: None\n",
      "05:15:32 |     learningrate: 1\n",
      "05:15:32 |     load_from_checkpoint: True\n",
      "05:15:32 |     log_every_n_secs: -1\n",
      "05:15:32 |     log_every_n_steps: 50\n",
      "05:15:32 |     loglevel: info\n",
      "05:15:32 |     lr_scheduler: reduceonplateau\n",
      "05:15:32 |     lr_scheduler_decay: 0.5\n",
      "05:15:32 |     lr_scheduler_patience: 3\n",
      "05:15:32 |     max_train_steps: -1\n",
      "05:15:32 |     max_train_time: 60.0\n",
      "05:15:32 |     metrics: default\n",
      "05:15:32 |     model: my_first_lstm\n",
      "05:15:32 |     model_file: my_first_lstm/model\n",
      "05:15:32 |     momentum: 0\n",
      "05:15:32 |     multitask_weights: [1]\n",
      "05:15:32 |     mutators: None\n",
      "05:15:32 |     nesterov: True\n",
      "05:15:32 |     no_cuda: False\n",
      "05:15:32 |     num_epochs: -1\n",
      "05:15:32 |     num_workers: 0\n",
      "05:15:32 |     nus: [0.7]\n",
      "05:15:32 |     optimizer: sgd\n",
      "05:15:32 |     override: \"{'model': 'my_first_lstm', 'model_file': 'my_first_lstm/model', 'task': 'cbt', 'batchsize': 3, 'validation_every_n_secs': 10.0, 'max_train_time': 60.0}\"\n",
      "05:15:32 |     parlai_home: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\n",
      "05:15:32 |     person_tokens: False\n",
      "05:15:32 |     rank_candidates: False\n",
      "05:15:32 |     save_after_valid: False\n",
      "05:15:32 |     save_every_n_secs: -1\n",
      "05:15:32 |     short_final_eval: False\n",
      "05:15:32 |     skip_generation: False\n",
      "05:15:32 |     special_tok_lst: None\n",
      "05:15:32 |     split_lines: False\n",
      "05:15:32 |     starttime: Dec05_03-49\n",
      "05:15:32 |     task: cbt\n",
      "05:15:32 |     temperature: 1.0\n",
      "05:15:32 |     tensorboard_log: False\n",
      "05:15:32 |     tensorboard_logdir: None\n",
      "05:15:32 |     text_truncate: None\n",
      "05:15:32 |     topk: 10\n",
      "05:15:32 |     topp: 0.9\n",
      "05:15:32 |     truncate: -1\n",
      "05:15:32 |     update_freq: 1\n",
      "05:15:32 |     use_reply: label\n",
      "05:15:32 |     validation_cutoff: 1.0\n",
      "05:15:32 |     validation_every_n_epochs: -1\n",
      "05:15:32 |     validation_every_n_secs: 10.0\n",
      "05:15:32 |     validation_every_n_steps: -1\n",
      "05:15:32 |     validation_max_exs: -1\n",
      "05:15:32 |     validation_metric: accuracy\n",
      "05:15:32 |     validation_metric_mode: None\n",
      "05:15:32 |     validation_patience: 10\n",
      "05:15:32 |     validation_share_agent: False\n",
      "05:15:32 |     verbose: False\n",
      "05:15:32 |     wandb_entity: None\n",
      "05:15:32 |     wandb_log: False\n",
      "05:15:32 |     wandb_name: None\n",
      "05:15:32 |     wandb_project: None\n",
      "05:15:32 |     warmup_rate: 0.0001\n",
      "05:15:32 |     warmup_updates: -1\n",
      "05:15:32 |     weight_decay: None\n",
      "05:15:33 | creating task(s): cbt\n",
      "05:15:33 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_train.txt\n",
      "05:15:40 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_train.txt\n",
      "05:15:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_train.txt\n",
      "05:16:00 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_train.txt\n",
      "05:16:25 | training...\n",
      "05:16:25 | max_train_time elapsed:3943.8686530590057s\n",
      "05:16:27 | Using CUDA\n",
      "05:16:27 | loading dictionary from my_first_lstm/model.dict\n",
      "05:16:27 | num words = 51210\n",
      "05:16:27 | Total parameters: 69,232,640 (69,232,640 trainable)\n",
      "05:16:27 | Loading existing model params from my_first_lstm/model\n",
      "05:16:28 | creating task(s): cbt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_valid_2000ex.txt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_valid_2000ex.txt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_valid_2000ex.txt\n",
      "05:16:28 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_valid_2000ex.txt\n",
      "05:16:28 | running eval: valid\n",
      "05:24:48 | eval completed in 499.79s\n",
      "05:24:48 | \u001b[1mvalid:\n",
      "           accuracy    bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs      f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all      .007875 7.892e-12 512.3  1536  8201       0          0 16.01 8000 .007938    .1936 2.013 9.903   1 6.035 32.22   \n",
      "   cbt:CN     .0010     1e-12 522.1                   0          0       2000   .0010          2.004 11.36                   \n",
      "   cbt:NE         0 6.767e-14   489                   0          0       2000  .00025          2.047 10.13                   \n",
      "   cbt:P      .0305  3.05e-11 528.9                   0          0       2000   .0305              2  7.11                   \n",
      "   cbt:V          0         0 509.2                   0          0       2000       0              2 11.02                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  \n",
      "   all          0          0 43190      .3975    .00775                   51 1542 8233  \n",
      "   cbt:CN       0          0 85669      .3787     .0010                                 \n",
      "   cbt:NE       0          0 25088      .3464         0                                 \n",
      "   cbt:P        0          0  1224      .4975     .0300                                 \n",
      "   cbt:V        0          0 60780      .3675         0\n",
      "\u001b[0m\n",
      "05:24:48 | creating task(s): cbt\n",
      "05:24:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_NE_test_2500ex.txt\n",
      "05:24:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_CN_test_2500ex.txt\n",
      "05:24:48 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_V_test_2500ex.txt\n",
      "05:24:49 | loading fbdialog data: C:\\Users\\hp\\anaconda3\\envs\\pytorch\\lib\\site-packages\\data\\CBT\\CBTest\\data\\cbtest_P_test_2500ex.txt\n",
      "05:24:49 | running eval: test\n",
      "05:35:27 | eval completed in 638.22s\n",
      "05:35:27 | \u001b[1mtest:\n",
      "           accuracy   bleu-4  clen  ctpb  ctps  ctrunc  ctrunclen  exps   exs    f1  gpu_mem  llen  loss  lr  ltpb  ltps  \\\n",
      "   all        .0088  8.8e-12 524.4  1572  8216       0          0 15.67 10000 .0088    .1938 2.029    10   1 6.083 31.79   \n",
      "   cbt:CN         0        0 539.4                   0          0        2500     0          2.004 11.25                   \n",
      "   cbt:NE         0        0 503.7                   0          0        2500     0           2.11  10.7                   \n",
      "   cbt:P      .0352 3.52e-11 537.1                   0          0        2500 .0352              2 7.159                   \n",
      "   cbt:V          0        0 517.3                   0          0        2500     0          2.001  10.9                   \n",
      "           ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb  tps  \n",
      "   all          0          0 44172      .3888     .0087                   51 1578 8248  \n",
      "   cbt:CN       0          0 76860      .3615         0                                 \n",
      "   cbt:NE       0          0 44272      .3062         0                                 \n",
      "   cbt:P        0          0  1286      .4996     .0348                                 \n",
      "   cbt:V        0          0 54270      .3880         0\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'cbt:NE/exs': SumMetric(2000),\n",
       "  'exs': SumMetric(8000),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0.00025),\n",
       "  'cbt:NE/bleu-4': BleuMetric(6.767e-14),\n",
       "  'cbt:NE/clen': AverageMetric(489),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.047),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.13),\n",
       "  'cbt:NE/ppl': PPLMetric(2.509e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.3464),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2000),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0.001),\n",
       "  'cbt:CN/f1': F1Metric(0.001),\n",
       "  'cbt:CN/bleu-4': BleuMetric(1e-12),\n",
       "  'cbt:CN/clen': AverageMetric(522.1),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.36),\n",
       "  'cbt:CN/ppl': PPLMetric(8.567e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3787),\n",
       "  'cbt:CN/token_em': AverageMetric(0.001),\n",
       "  'cbt:V/exs': SumMetric(2000),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(509.2),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(11.02),\n",
       "  'cbt:V/ppl': PPLMetric(6.078e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.3675),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2000),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0305),\n",
       "  'cbt:P/f1': F1Metric(0.0305),\n",
       "  'cbt:P/bleu-4': BleuMetric(3.05e-11),\n",
       "  'cbt:P/clen': AverageMetric(528.9),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(7.11),\n",
       "  'cbt:P/ppl': PPLMetric(1224),\n",
       "  'cbt:P/token_acc': AverageMetric(0.4975),\n",
       "  'cbt:P/token_em': AverageMetric(0.03),\n",
       "  'accuracy': MacroAverageMetric(0.007875),\n",
       "  'f1': MacroAverageMetric(0.007938),\n",
       "  'bleu-4': MacroAverageMetric(7.892e-12),\n",
       "  'clen': MacroAverageMetric(512.3),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.013),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(9.903),\n",
       "  'ppl': MacroAverageMetric(4.319e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3975),\n",
       "  'token_em': MacroAverageMetric(0.00775),\n",
       "  'exps': GlobalTimerMetric(16.01),\n",
       "  'ltpb': GlobalAverageMetric(6.035),\n",
       "  'ltps': GlobalTimerMetric(32.22),\n",
       "  'ctpb': GlobalAverageMetric(1536),\n",
       "  'ctps': GlobalTimerMetric(8201),\n",
       "  'tpb': GlobalAverageMetric(1542),\n",
       "  'tps': GlobalTimerMetric(8233),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1936),\n",
       "  'total_train_updates': GlobalFixedMetric(51)},\n",
       " {'cbt:NE/exs': SumMetric(2500),\n",
       "  'exs': SumMetric(1e+04),\n",
       "  'cbt:NE/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:NE/f1': F1Metric(0),\n",
       "  'cbt:NE/bleu-4': BleuMetric(0),\n",
       "  'cbt:NE/clen': AverageMetric(503.7),\n",
       "  'cbt:NE/ctrunc': AverageMetric(0),\n",
       "  'cbt:NE/ctrunclen': AverageMetric(0),\n",
       "  'cbt:NE/llen': AverageMetric(2.11),\n",
       "  'cbt:NE/ltrunc': AverageMetric(0),\n",
       "  'cbt:NE/ltrunclen': AverageMetric(0),\n",
       "  'cbt:NE/loss': AverageMetric(10.7),\n",
       "  'cbt:NE/ppl': PPLMetric(4.427e+04),\n",
       "  'cbt:NE/token_acc': AverageMetric(0.3062),\n",
       "  'cbt:NE/token_em': AverageMetric(0),\n",
       "  'cbt:CN/exs': SumMetric(2500),\n",
       "  'cbt:CN/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:CN/f1': F1Metric(0),\n",
       "  'cbt:CN/bleu-4': BleuMetric(0),\n",
       "  'cbt:CN/clen': AverageMetric(539.4),\n",
       "  'cbt:CN/ctrunc': AverageMetric(0),\n",
       "  'cbt:CN/ctrunclen': AverageMetric(0),\n",
       "  'cbt:CN/llen': AverageMetric(2.004),\n",
       "  'cbt:CN/ltrunc': AverageMetric(0),\n",
       "  'cbt:CN/ltrunclen': AverageMetric(0),\n",
       "  'cbt:CN/loss': AverageMetric(11.25),\n",
       "  'cbt:CN/ppl': PPLMetric(7.686e+04),\n",
       "  'cbt:CN/token_acc': AverageMetric(0.3615),\n",
       "  'cbt:CN/token_em': AverageMetric(0),\n",
       "  'cbt:V/exs': SumMetric(2500),\n",
       "  'cbt:V/accuracy': ExactMatchMetric(0),\n",
       "  'cbt:V/f1': F1Metric(0),\n",
       "  'cbt:V/bleu-4': BleuMetric(0),\n",
       "  'cbt:V/clen': AverageMetric(517.3),\n",
       "  'cbt:V/ctrunc': AverageMetric(0),\n",
       "  'cbt:V/ctrunclen': AverageMetric(0),\n",
       "  'cbt:V/llen': AverageMetric(2.001),\n",
       "  'cbt:V/ltrunc': AverageMetric(0),\n",
       "  'cbt:V/ltrunclen': AverageMetric(0),\n",
       "  'cbt:V/loss': AverageMetric(10.9),\n",
       "  'cbt:V/ppl': PPLMetric(5.427e+04),\n",
       "  'cbt:V/token_acc': AverageMetric(0.388),\n",
       "  'cbt:V/token_em': AverageMetric(0),\n",
       "  'cbt:P/exs': SumMetric(2500),\n",
       "  'cbt:P/accuracy': ExactMatchMetric(0.0352),\n",
       "  'cbt:P/f1': F1Metric(0.0352),\n",
       "  'cbt:P/bleu-4': BleuMetric(3.52e-11),\n",
       "  'cbt:P/clen': AverageMetric(537.1),\n",
       "  'cbt:P/ctrunc': AverageMetric(0),\n",
       "  'cbt:P/ctrunclen': AverageMetric(0),\n",
       "  'cbt:P/llen': AverageMetric(2),\n",
       "  'cbt:P/ltrunc': AverageMetric(0),\n",
       "  'cbt:P/ltrunclen': AverageMetric(0),\n",
       "  'cbt:P/loss': AverageMetric(7.159),\n",
       "  'cbt:P/ppl': PPLMetric(1286),\n",
       "  'cbt:P/token_acc': AverageMetric(0.4996),\n",
       "  'cbt:P/token_em': AverageMetric(0.0348),\n",
       "  'accuracy': MacroAverageMetric(0.0088),\n",
       "  'f1': MacroAverageMetric(0.0088),\n",
       "  'bleu-4': MacroAverageMetric(8.8e-12),\n",
       "  'clen': MacroAverageMetric(524.4),\n",
       "  'ctrunc': MacroAverageMetric(0),\n",
       "  'ctrunclen': MacroAverageMetric(0),\n",
       "  'llen': MacroAverageMetric(2.029),\n",
       "  'ltrunc': MacroAverageMetric(0),\n",
       "  'ltrunclen': MacroAverageMetric(0),\n",
       "  'loss': MacroAverageMetric(10),\n",
       "  'ppl': MacroAverageMetric(4.417e+04),\n",
       "  'token_acc': MacroAverageMetric(0.3888),\n",
       "  'token_em': MacroAverageMetric(0.0087),\n",
       "  'exps': GlobalTimerMetric(15.67),\n",
       "  'ltpb': GlobalAverageMetric(6.083),\n",
       "  'ltps': GlobalTimerMetric(31.79),\n",
       "  'ctpb': GlobalAverageMetric(1572),\n",
       "  'ctps': GlobalTimerMetric(8216),\n",
       "  'tpb': GlobalAverageMetric(1578),\n",
       "  'tps': GlobalTimerMetric(8248),\n",
       "  'lr': GlobalAverageMetric(1),\n",
       "  'gpu_mem': GlobalAverageMetric(0.1938),\n",
       "  'total_train_updates': GlobalFixedMetric(51)})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainModel.main(\n",
    "    model='my_first_lstm',\n",
    "    model_file='my_first_lstm/model',\n",
    "    task='cbt',\n",
    "    batchsize=3,\n",
    "    validation_every_n_secs=10,\n",
    "    max_train_time=60,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TDJFWJk2JL_C",
    "cBfO781TltNR"
   ],
   "name": "PC5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
